import streamlit as st
import openai
import numpy as np
import pandas as pd
from PIL import Image as PILImage
from search import create_index_faiss, find_closest_vectors_faiss
from openai import OpenAI
import os
import subprocess
import io
import traceback
import shutil
import sqlite3
import time
import uuid
from pathlib import Path
import xml.etree.ElementTree as ET
import sys
from datetime import datetime, timedelta
import cv2
from yolox.yolox_onnx_predictor import YOLOXONNXPredictor
import json
import re
from dotenv import load_dotenv
import boto3
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy import Column, Integer, String, JSON
from sqlalchemy import func, text, or_
from sqlalchemy.dialects.postgresql import JSONB


load_dotenv()

DB_PATH = "annotation_data.db"

# S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
S3_BUCKET_NAME = os.getenv("AWS_S3_BUCKET_NAME")
s3 = boto3.client(
    "s3",
    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    region_name=os.getenv("AWS_S3_REGION")
)
# --- â˜… è¨­å®šã“ã“ã¾ã§ â˜… ---

TEMP_SESSIONS_BASE_DIR = "temp_sessions"
OLD_SESSION_MAX_AGE_HOURS = 24 # èµ·å‹•æ™‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ï¼ˆä¾‹: 24æ™‚é–“ï¼‰
CURRENT_SESSION_TIMEOUT_SECONDS = 3600 # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆä¾‹: 1æ™‚é–“ = 3600ç§’ï¼‰

# äº‹å‰å®šç¾©ãƒ¢ãƒ‡ãƒ«
PRESET_MODELS = {
    "ä¿¡å·æ¤œå‡º":{
        "path":"yolox/onnx_models/tlr_car_ped_yolox_s_batch_1.onnx", # ãƒãƒƒãƒã‚µã‚¤ã‚º1ã®ãƒ¢ãƒ‡ãƒ«ã‚’æƒ³å®š
        "input_shape":"416,416",
        "class_names":[ "pedestrian_traffic_light", "traffic_light"]
    },
}

# --- SQLAlchemyãƒ¢ãƒ‡ãƒ«å®šç¾© ---

# ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ãŒç¶™æ‰¿ã™ã‚‹ãŸã‚ã®Baseã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
Base = declarative_base()

# ç”»åƒæƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«
class Image(Base):
    __tablename__ = 'images'
    id = Column(Integer, primary_key=True, index=True)
    filename = Column(String, unique=True, index=True)
    s3_key = Column(String, unique=True)
    label = Column(String, nullable=True)
    annotations = Column(JSON, nullable=True)
    width = Column(Integer, nullable=True)
    height = Column(Integer, nullable=True)

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šè¨­å®š ---
# å…ƒã®SQLiteæ¥ç¶šè¨­å®š
# DATABASE_FILE = "annotation.db"
# engine = create_engine(f"sqlite:///{DATABASE_FILE}")

# æ–°ã—ã„PostgreSQLã¸ã®æ¥ç¶šè¨­å®š
DATABASE_URL = os.getenv("DATABASE_URL")
if DATABASE_URL is None:
    st.error("DATABASE_URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

engine = create_engine(DATABASE_URL)

# --- ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ ---
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆå›ä½œæˆã™ã‚‹
Base.metadata.create_all(engine)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def parse_pascal_voc_xml(xml_path):
    tree = ET.parse(xml_path)
    root = tree.getroot()
    objects = []
    for obj in root.findall('object'):
        label = obj.find('name').text
        bbox = obj.find('bndbox')
        xmin = int(bbox.find('xmin').text)
        ymin = int(bbox.find('ymin').text)
        xmax = int(bbox.find('xmax').text)
        ymax = int(bbox.find('ymax').text)
        objects.append({'label_name': label, 'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax})
    return objects

def get_classes_from_xml_dir(xml_dir_path):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®PASCAL VOC XMLã‹ã‚‰ã‚¯ãƒ©ã‚¹åã‚’æŠ½å‡ºã™ã‚‹"""
    annotated_classes = set()
    if not os.path.isdir(xml_dir_path):
        return annotated_classes
    
    for filename in os.listdir(xml_dir_path):
        if filename.endswith(".xml"):
            xml_file_path = os.path.join(xml_dir_path, filename)
            try:
                tree = ET.parse(xml_file_path)
                root = tree.getroot()
                for obj in root.findall("object"):
                    name_element = obj.find("name")
                    if name_element is not None and name_element.text:
                        annotated_classes.add(name_element.text.strip())
            except ET.ParseError:
                # ä¸æ­£ãªXMLã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
    return annotated_classes

def format_caption(rank, description):
    return f"**Rank {rank}:**\n{description}"

@st.cache_data
def search_images_in_db(search_term):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ç”»åƒã‚’æ¤œç´¢ã—ã€çµæœã‚’è¿”ã™ã€‚çµæœã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ã€‚"""
    db = SessionLocal()
    try:
        query = db.query(Image)
        if search_term:
            search_pattern = f'%{search_term}%'
            from sqlalchemy import or_
            query = query.filter(
                or_(
                    Image.filename.ilike(search_pattern),
                    Image.label.ilike(search_pattern)
                )
            )
        return query.order_by(Image.id.desc()).all()
    finally:
        db.close()

def run_image_search_app():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ã•ã‚ŒãŸç”»åƒã®ä¸€è¦§è¡¨ç¤ºã¨æ¤œç´¢ã‚’è¡Œã†ãƒšãƒ¼ã‚¸"""
    st.header("ç™»éŒ²æ¸ˆã¿ç”»åƒã®æ¤œç´¢ãƒ»é–²è¦§")

    # --- æ¤œç´¢UI ---
    search_term = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åã‚„åˆ†é¡ãƒ©ãƒ™ãƒ«ã§æ¤œç´¢:")

    # --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ç”»åƒæƒ…å ±ã‚’å–å¾— ---
    all_images = search_images_in_db(search_term)

    if not all_images:
        st.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
        
    st.write(f"**{len(all_images)}** ä»¶ã®ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
    st.markdown("---")

    # --- ç”»åƒä¸€è¦§ã®è¡¨ç¤º ---
    if 'temp_dir_session' not in st.session_state or not os.path.exists(st.session_state.temp_dir_session):
        st.session_state.temp_dir_session = setup_session_temp_dir()
    temp_dir = st.session_state.temp_dir_session

    cols = st.columns(4) # 4åˆ—ã§è¡¨ç¤º
    for i, img_obj in enumerate(all_images):
        with cols[i % 4]:
            image_bytes = get_image_bytes_from_s3(img_obj.s3_key)
            
            if image_bytes:
                st.image(image_bytes, use_column_width=True)
                st.markdown(f"**{img_obj.filename}**")
                # DBã‹ã‚‰å–å¾—ã—ãŸæœ€æ–°ã®åˆ†é¡ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤º
                st.caption(f"åˆ†é¡: {img_obj.label or '(æœªè¨­å®š)'}")
                anno_count = len(img_obj.annotations) if img_obj.annotations else 0
                st.caption(f"ç‰©ä½“æ•°: {anno_count}")
            else:
                st.warning(f"è¡¨ç¤ºä¸å¯:\n{img_obj.filename}")
                                    
def setup_session_temp_dir():
    base_temp_path = Path(TEMP_SESSIONS_BASE_DIR)
    base_temp_path.mkdir(exist_ok=True)
    session_dir_name = f"session_{str(uuid.uuid4())[:8]}"
    session_temp_path = base_temp_path / session_dir_name
    session_temp_path.mkdir(exist_ok=True, parents=True)
    
    if 'last_activity_time' in st.session_state:
        st.session_state.last_activity_time = time.time()
    else:
        st.session_state.last_activity_time = time.time()
    return str(session_temp_path)

def cleanup_session_temp_dir(session_dir_path):
    if session_dir_path and os.path.exists(session_dir_path):
        try:
            shutil.rmtree(session_dir_path)
        except Exception as e:
            st.error(f"ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {session_dir_path}, Error: {e}")

def generate_pascal_voc_xml(images_to_export: list[Image], tmpdir_path: Path):
    """
    æ¤œå‡ºçµæœã‹ã‚‰PASCAL VOC XMLæ–‡å­—åˆ—ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    objects: [{'label_name': str, 'xmin': int, ...}, ...]
    """
    missing_files = []

    image_dir = tmpdir_path / "images"
    annotations_dir = tmpdir_path / "annotations"

    for image_obj in images_to_export:
        local_image_path = get_image_from_s3(image_obj.s3_key, str(tmpdir_path))
        if not local_image_path:
            missing_files.append(image_obj.filename)
            continue

        try:
            with PILImage.open(local_image_path) as img:
                width, height = img.size
        except Exception as e:
            st.warning(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«{image_obj.filename}ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚{e}")
            missing_files.append(image_obj.filename)
            continue

        shutil.copy(local_image_path, image_dir / image_obj.filename)

        detected_objects = image_obj.annotations if image_obj.annotations else []
        xml_content = generate_pascal_voc_xml_content(image_obj.filename, local_image_path, width, height, detected_objects)
        xml_filename = Path(image_obj.filename).stem + ".xml"
        with open(annotations_dir / xml_filename, "w", encoding="utf-8") as f:
            f.write(xml_content)

    return len(images_to_export) - len(missing_files), missing_files

def generate_coco_dataset(images_to_export: list[Image], tmpdir_path: Path):
    """
    æ¤œå‡ºçµæœã‹ã‚‰COCO JSONæ–‡å­—åˆ—ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    
    # å…¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’åé›†
    all_labels = sorted(list(set(
        obj["label_name"] for img in images_to_export if img.annotations for obj in img.annotations

    )))
    categories = [{"id": i+1, "name": label, "supercategory": "none"} for i, label in enumerate(all_labels)]
    category_map = {cat["name"]: cat["id"] for cat in categories}

    coco_output = {
        "info": {"description": "Generated by Annotation Tool", "version": "1.0", "year": datetime.now().year, "date_created": datetime.now().strftime('%Y/%m/%d')},
        "licenses": [], "images": [], "annotations": [], "categories": categories
    }

    annotation_id_counter = 1
    missing_files = []
    images_dir = tmpdir_path / "images"

    for image_obj in images_to_export:
        local_image_path = get_image_from_s3(image_obj.s3_key, str(tmpdir_path))
        if not local_image_path:
            missing_files.append(image_obj.filename)
            continue
            
        try:
            with PILImage.open(local_image_path) as img:
                width, height = img.size
        except Exception as e:
            st.warning(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ« {image_obj.filename} ã‚’é–‹ã‘ã¾ã›ã‚“: {e}")
            missing_files.append(image_obj.filename)
            continue

        shutil.copy(local_image_path, images_dir / image_obj.filename)
        
        image_coco_id = len(coco_output["images"]) + 1
        coco_output["images"].append({"id": image_coco_id, "file_name": image_obj.filename, "width": width, "height": height})
        
        if image_obj.annotations:
            for anno in image_obj.annotations:
                xmin, ymin, xmax, ymax = anno['xmin'], anno['ymin'], anno['xmax'], anno['ymax']
                bbox_width, bbox_height = xmax - xmin, ymax - ymin
                if anno['label_name'] in category_map:
                    coco_output["annotations"].append({
                        "id": annotation_id_counter, "image_id": image_coco_id, 
                        "category_id": category_map[anno['label_name']], 
                        "bbox": [xmin, ymin, bbox_width, bbox_height], 
                        "area": bbox_width * bbox_height, "iscrowd": 0, "segmentation": []
                    })
                    annotation_id_counter += 1
    
    with open(tmpdir_path / "annotations" / "instances.json", "w") as f:
        json.dump(coco_output, f, indent=4)
        
    return len(images_to_export) - len(missing_files), missing_files

def generate_yolo_dataset(images_to_export: list[Image], tmpdir_path: Path):
    """YOLO Darknetå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã€çµ±è¨ˆæƒ…å ±ã‚’è¿”ã™"""
    st.info("YOLO Darknetå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
    
    all_labels = sorted(list(set(
        obj['label_name'] 
        for img in images_to_export if img.annotations 
        for obj in img.annotations
    )))
    class_map = {label: i for i, label in enumerate(all_labels)}
    with open(tmpdir_path / "classes.txt", "w") as f:
        for label in all_labels: f.write(f"{label}\n")

    missing_files = []
    images_dir = tmpdir_path / "images"
    annotations_dir = tmpdir_path / "annotations"

    for image_obj in images_to_export:
        local_image_path = get_image_from_s3(image_obj.s3_key, str(tmpdir_path))
        if not local_image_path:
            missing_files.append(image_obj.filename)
            continue

        try:
            with PILImage.open(local_image_path) as img:
                img_width, img_height = img.size
        except Exception as e:
            st.warning(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ« {image_obj.filename} ã‚’é–‹ã‘ã¾ã›ã‚“: {e}")
            missing_files.append(image_obj.filename)
            continue
            
        shutil.copy(local_image_path, images_dir / image_obj.filename)
        
        yolo_lines = []
        if image_obj.annotations:
            for anno in image_obj.annotations:
                class_id = class_map[anno['label_name']]
                xmin, ymin, xmax, ymax = anno['xmin'], anno['ymin'], anno['xmax'], anno['ymax']
                x_center = (xmin + xmax) / 2 / img_width
                y_center = (ymin + ymax) / 2 / img_height
                width = (xmax - xmin) / img_width
                height = (ymax - ymin) / img_height
                yolo_lines.append(f"{class_id} {x_center} {y_center} {width} {height}")
        
        txt_filename = Path(image_obj.filename).stem + ".txt"
        with open(annotations_dir / txt_filename, "w") as f:
            f.write("\n".join(yolo_lines))
            
    return len(images_to_export) - len(missing_files), missing_files

def get_image_from_s3(s3_key, temp_dir):
    """S3ã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿”ã™"""
    if not s3_key:
        return None
    try:
        local_path = os.path.join(temp_dir, os.path.basename(s3_key))
        s3.download_file(S3_BUCKET_NAME, s3_key, local_path)
        return local_path
    except Exception as e:
        st.warning(f"S3ã‹ã‚‰ã®ç”»åƒå–å¾—ã«å¤±æ•—: {s3_key}, Error: {e}")
        return None

@st.cache_data
def get_image_bytes_from_s3(s3_key):
    """S3ã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã€‚çµæœã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ã€‚"""
    if not s3_key:
        return None
    try:
        s3_object = s3.get_object(Bucket=S3_BUCKET_NAME, Key=s3_key)
        return s3_object['Body'].read()
    except Exception as e:
        st.warning(f"S3ã‹ã‚‰ã®ç”»åƒå–å¾—ã«å¤±æ•—: {s3_key}, Error: {e}")
        return None

@st.cache_data
def get_bytes_from_uploaded_file(uploaded_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã€‚çµæœã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ã€‚"""
    return uploaded_file.getvalue()

def generate_pascal_voc_xml_content(filename, path, width, height, objects):
    """
    æ¤œå‡ºçµæœã‹ã‚‰PASCAL VOC XMLã®æ–‡å­—åˆ—ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    objects: [{'label_name': str, 'xmin': int, ...}, ...]
    """
    annotation = ET.Element('annotation')
    ET.SubElement(annotation, 'folder').text = "images"
    ET.SubElement(annotation, 'filename').text = filename
    ET.SubElement(annotation, 'path').text = str(path)
    source = ET.SubElement(annotation, 'source')
    ET.SubElement(source, 'database').text = 'Unknown'
    
    size = ET.SubElement(annotation, 'size')
    ET.SubElement(size, 'width').text = str(width)
    ET.SubElement(size, 'height').text = str(height)
    ET.SubElement(size, 'depth').text = '3'
    ET.SubElement(annotation, 'segmented').text = '0'

    # objectsãŒNoneã‚„ç©ºãƒªã‚¹ãƒˆã®å ´åˆã§ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã‚ˆã†ã«ãƒã‚§ãƒƒã‚¯
    if objects:
        for obj_dict in objects:
            obj = ET.SubElement(annotation, 'object')
            ET.SubElement(obj, 'name').text = obj_dict['label_name']
            ET.SubElement(obj, 'pose').text = 'Unspecified'
            ET.SubElement(obj, 'truncated').text = '0'
            ET.SubElement(obj, 'difficult').text = '0'
            bndbox = ET.SubElement(obj, 'bndbox')
            ET.SubElement(bndbox, 'xmin').text = str(obj_dict['xmin'])
            ET.SubElement(bndbox, 'ymin').text = str(obj_dict['ymin'])
            ET.SubElement(bndbox, 'xmax').text = str(obj_dict['xmax'])
            ET.SubElement(bndbox, 'ymax').text = str(obj_dict['ymax'])

    # ElementTreeã‹ã‚‰æ•´å½¢ã•ã‚ŒãŸXMLæ–‡å­—åˆ—ã‚’ç”Ÿæˆ
    ET.indent(annotation, space="  ")
    xml_string = ET.tostring(annotation, encoding='unicode')
    
    return f'<?xml version="1.0" ?>\n{xml_string}'

def convert_image_to_vector():
    # --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†å¤‰æ•°ã®åˆæœŸåŒ– ---
    if 'staged_files' not in st.session_state:
        st.session_state.staged_files = {} # filename: UploadedFile
    if 'annotation_data' not in st.session_state:
        st.session_state.annotation_data = {} # filename: {annotations: [], label: ""}

    # --- ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™ ---
    if 'temp_dir_session' not in st.session_state or not os.path.exists(st.session_state.temp_dir_session):
        st.session_state.temp_dir_session = setup_session_temp_dir()
    temp_dir = st.session_state.temp_dir_session
    local_image_storage_dir = os.path.join(temp_dir, "images_for_labeling")
    open_labeling_output_dir_base = os.path.join(temp_dir, "annotations_output")
    open_labeling_pascal_voc_output_dir = os.path.join(open_labeling_output_dir_base, "PASCAL_VOC")
    os.makedirs(local_image_storage_dir, exist_ok=True)
    os.makedirs(open_labeling_pascal_voc_output_dir, exist_ok=True) 

    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    st.header("1. ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["png", "jpg", "jpeg"], accept_multiple_files=True, key="file_uploader_main")
    if uploaded_files:
        new_files_added = False
        for f in uploaded_files:
            if f.name not in st.session_state.staged_files:
                st.session_state.staged_files[f.name] = f
                # å¯¾å¿œã™ã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚‚åˆæœŸåŒ–
                st.session_state.annotation_data[f.name] = {"annotations": [], "label": ""}
                new_files_added = True
        if new_files_added:
            st.rerun()

    if not st.session_state.staged_files:
        st.info("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ã“ã“ã«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        st.stop()

    # --- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ---
    st.subheader(f"{len(st.session_state.staged_files)} ä»¶ã®ç”»åƒãŒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ä¸­")
    with st.expander("ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’é–‹ã/é–‰ã˜ã‚‹"):
        cols = st.columns(4)
        for i, (filename, file_data) in enumerate(st.session_state.staged_files.items()):
            with cols[i % 4]:
                image_bytes = get_bytes_from_uploaded_file(file_data)
                st.image(image_bytes, caption=filename, use_column_width=True)
    
    st.markdown("---")

    # --- 2. è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ---
    st.header("2. è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ (ä»»æ„)")
    auto_annotate_method = st.radio("æ–¹æ³•ã‚’é¸æŠ", ("ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨", "ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"), key="auto_annotate_method_radio")

    onnx_model_bytes, class_names, input_shape = None, None, "640,640"
    if auto_annotate_method == "ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨" and PRESET_MODELS:
        model_name = st.selectbox("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ", options=list(PRESET_MODELS.keys()))
        model_info = PRESET_MODELS[model_name]
                    if os.path.exists(model_info["path"]):
                        with open(model_info["path"], "rb") as f:
                onnx_model_bytes = f.read()
            class_names = model_info.get("class_names")
            input_shape = model_info.get("input_shape", "640,640")
                    else:
            st.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_info['path']}")
    elif auto_annotate_method == "ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        f = st.file_uploader("ONNXãƒ¢ãƒ‡ãƒ« (.onnx)", type=["onnx"])
        if f: onnx_model_bytes = f.read()

    confidence_threshold = st.slider("ä¿¡é ¼åº¦ã®ã—ãã„å€¤", 0.0, 1.0, 0.3, 0.05)
    
    if st.button("è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"):
        if onnx_model_bytes:
            try:
                predictor = YOLOXONNXPredictor(model_bytes=onnx_model_bytes, input_shape_str=input_shape, class_names=class_names)
                with st.spinner("è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œä¸­..."):
                    for filename, file_data in st.session_state.staged_files.items():
                        image_bytes = get_bytes_from_uploaded_file(file_data)
                        np_array = np.frombuffer(image_bytes, np.uint8)
                        cv_image = cv2.imdecode(np_array, cv2.IMREAD_COLOR)
                        detected = predictor.predict(cv_image, score_thr=confidence_threshold)
                        # çµæœã‚’XMLã¨ã—ã¦ä¿å­˜
                        pil_img = PILImage.open(io.BytesIO(image_bytes))
                        xml_content = generate_pascal_voc_xml_content(filename, "s3_path_placeholder", pil_img.width, pil_img.height, detected)
                        with open(os.path.join(open_labeling_pascal_voc_output_dir, f"{Path(filename).stem}.xml"), "w") as f:
                            f.write(xml_content)
                st.success("è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ä¸‹è¨˜OpenLabelingã§çµæœã‚’ç¢ºèªãƒ»ä¿®æ­£ã§ãã¾ã™ã€‚")
            except Exception as e:
                st.error(f"è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            st.warning("ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã¾ãŸã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    
    st.markdown("---")
    
    # --- 3. æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ---
    st.header("3. æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ (OpenLabeling)")
    st.info("è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœã‚„ã€æ–°è¦ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã“ã®ãƒ„ãƒ¼ãƒ«ã§ç·¨é›†ã—ã¾ã™ã€‚")
    
    # ã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆã®æº–å‚™
    class_list_file_path = os.path.abspath(os.path.join("OpenLabeling", "main", "class_list.txt"))
    default_classes_str = "object\nperson\nvehicle"
    try:
        if os.path.exists(class_list_file_path):
            with open(class_list_file_path, "r") as f: default_classes_str = f.read()
    except Exception: pass
    
    edited_classes_str = st.text_area("ã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆ (1è¡Œ1ã‚¯ãƒ©ã‚¹)", value=default_classes_str, height=150)

    if st.button("OpenLabelingã‚’èµ·å‹•"):
        # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ä¸­ã®ç”»åƒã‚’OpenLabelingã®å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€ã«æ›¸ãå‡ºã™
        for filename, file_data in st.session_state.staged_files.items():
            with open(os.path.join(local_image_storage_dir, filename), "wb") as f:
                f.write(get_bytes_from_uploaded_file(file_data))

        # ã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(class_list_file_path, "w") as f: f.write(edited_classes_str)

        command = [sys.executable, os.path.abspath("OpenLabeling/main/main.py"), "--input_dir", os.path.abspath(local_image_storage_dir), "--output_dir", os.path.abspath(open_labeling_output_dir_base), "--draw-from-PASCAL-files"]
        subprocess.Popen(command, cwd=os.path.abspath("OpenLabeling/main"))
        st.success("OpenLabelingãŒåˆ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§èµ·å‹•ã—ã¾ã—ãŸã€‚")
        
        st.markdown("---")
    
    # --- 4. åˆ†é¡ãƒ©ãƒ™ãƒ«å…¥åŠ›ã¨æœ€çµ‚ä¿å­˜ ---
    st.header("4. åˆ†é¡ãƒ©ãƒ™ãƒ«å…¥åŠ›ã¨DBã¸ã®ä¿ç‰©ä½“ãƒ©ãƒ™ãƒ«ã®é›†è¨ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: (psycopg2.errors.UndefinedFunction) function jsonb_array_elements(json) does not exist LINE 2: FROM (SELECT images.id AS image_id, jsonb_array_elements(ima... ^ HINT: No function matches the given name and argument types. You might need to add explicit type casts.

[SQL: SELECT anon_1.label_name AS anon_1_label_name, count(distinct(anon_1.image_id)) AS count_1 FROM (SELECT images.id AS image_id, jsonb_array_elements(images.annotations) ->> %(jsonb_array_elements_1)s AS label_name FROM images WHERE images.annotations IS NOT NULL AND jsonb_array_length(images.annotations) > %(jsonb_array_length_1)s) AS anon_1 GROUP BY anon_1.label_name ORDER BY anon_1.label_name] [parameters: {'jsonb_array_elements_1': 'label_name', 'jsonb_array_length_1': 0}] (Background on this error at: https://sqlalche.me/e/20/f405)
                for filename, file_data in st.session_state.staged_files.items():
                    # (1) ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³XMLã‚’èª­ã¿è¾¼ã‚€
                    xml_path = os.path.join(open_labeling_pascal_voc_output_dir, f"{Path(filename).stem}.xml")
                    if os.path.exists(xml_path):
                        st.session_state.annotation_data[filename]["annotations"] = parse_pascal_voc_xml(xml_path)
                    
                    # (2) S3ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                    s3_key = f"images/{uuid.uuid4()}_{filename}"
                    file_data.seek(0)
                    s3.upload_fileobj(file_data, S3_BUCKET_NAME, s3_key, ExtraArgs={'ContentType': file_data.type})

                    # (3) DBã¸ä¿å­˜ (SQLAlchemyä½¿ç”¨)
                    pil_img = PILImage.open(file_data)
                    
                    # å­˜åœ¨ç¢ºèª (Upsert)
                    image_obj = db.query(Image).filter(Image.s3_key == s3_key).first()
                    if image_obj: # æ›´æ–°
                        image_obj.filename = filename
                        image_obj.width = pil_img.width
                        image_obj.height = pil_img.height
                        image_obj.label = st.session_state.annotation_data[filename]["label"] or None
                        image_obj.annotations = st.session_state.annotation_data[filename]["annotations"]
                    else: # æ–°è¦ä½œæˆ
                        image_obj = Image(
                            filename=filename,
                            s3_key=s3_key,
                            width=pil_img.width,
                            height=pil_img.height,
                            label=st.session_state.annotation_data[filename]["label"] or None,
                            annotations=st.session_state.annotation_data[filename]["annotations"]
                        )
                        db.add(image_obj)
                
                db.commit()
                st.success(f"{len(st.session_state.staged_files)}ä»¶ã®ç”»åƒã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                # æ­£å¸¸ã«çµ‚äº†ã—ãŸã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªã‚¢
                st.session_state.staged_files.clear()
                st.session_state.annotation_data.clear()
                st.rerun()

                except Exception as e:
                db.rollback()
                st.error(f"ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.error(traceback.format_exc())
            finally:
                db.close()

    if st.session_state.staged_files:
        if st.button("ç¾åœ¨ã®å‡¦ç†ã‚’å…¨ã¦ã‚¯ãƒªã‚¢"):
            st.session_state.staged_files.clear()
            st.session_state.annotation_data.clear()
        st.rerun()

def cleanup_old_session_dirs():
    base_path = Path(TEMP_SESSIONS_BASE_DIR)
    if not base_path.exists():
        return
    now = datetime.now()
    for item in base_path.iterdir():
        if item.is_dir() and item.name.startswith("session_"):
            try:
                dir_modified_time = datetime.fromtimestamp(item.stat().st_mtime)
                if now - dir_modified_time > timedelta(hours=OLD_SESSION_MAX_AGE_HOURS):
                    shutil.rmtree(item)
            except Exception:
                pass 

def check_session_timeout():
    if 'last_activity_time' not in st.session_state:
        st.session_state.last_activity_time = time.time()
        return False

    if time.time() - st.session_state.last_activity_time > CURRENT_SESSION_TIMEOUT_SECONDS:
        current_session_dir_name = "ä¸æ˜ãªã‚»ãƒƒã‚·ãƒ§ãƒ³"
        if 'temp_dir_session' in st.session_state and st.session_state.temp_dir_session:
            current_session_dir_name = os.path.basename(st.session_state.temp_dir_session)
            cleanup_session_temp_dir(st.session_state.temp_dir_session)
        
        st.session_state.clear()
        st.session_state.temp_dir_session = setup_session_temp_dir()
        st.session_state.current_page = "ç”»åƒã‚’ç™»éŒ²ã™ã‚‹"
        
        st.info(f"é•·æ™‚é–“æ“ä½œãŒãªã‹ã£ãŸãŸã‚ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ ({current_session_dir_name}) ã‚’ã‚¯ãƒªã‚¢ã—ã€æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
        st.rerun()
        return True
    return False

def parse_label_from_option(option_str):
    """ "label (123æš)" ã¨ã„ã†å½¢å¼ã®æ–‡å­—åˆ—ã‹ã‚‰ãƒ©ãƒ™ãƒ«åã ã‘ã‚’æŠ½å‡ºã™ã‚‹ """
    match = re.match(r"^(.*) \(\d+æš\)$", option_str)
    if match:
        return match.group(1)
    return option_str

def create_dataset_page():
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚ã‚¿ãƒ–UIã‚’ä½¿ç”¨ã€‚"""
    st.header("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
    
    tab1, tab2 = st.tabs(["ç‰©ä½“æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", "ç”»åƒåˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"])

    # --- ç‰©ä½“æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã‚¿ãƒ– ---
    with tab1:
        st.info("ç‰¹å®šã®ç‰©ä½“ãƒ©ãƒ™ãƒ«ãŒå«ã¾ã‚Œã‚‹ç”»åƒã‚’æŠ½å‡ºã—ã€é¸æŠã—ãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚")
        
        db = SessionLocal()
        try:
            # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æŒã¤ç”»åƒã‹ã‚‰ãƒ©ãƒ™ãƒ«åã¨ã€ãã®ãƒ©ãƒ™ãƒ«ã‚’æŒã¤ç”»åƒã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ•°ã‚’é›†è¨ˆ
            subquery = db.query(
                Image.id.label("image_id"),
                func.jsonb_array_elements(Image.annotations).op('->>')('label_name').label("label_name")
            ).filter(Image.annotations != None, Image.annotations != text("'[]'")).subquery()
            query = db.query(
                subquery.c.label_name,
                func.count(func.distinct(subquery.c.image_id))
            ).group_by(subquery.c.label_name).order_by(subquery.c.label_name)
            available_labels = query.all()
        except Exception as e:
            st.error(f"ç‰©ä½“ãƒ©ãƒ™ãƒ«ã®é›†è¨ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            available_labels = []
        finally:
            db.close()

        if not available_labels:
            st.warning("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç‰©ä½“ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            dataset_format = st.selectbox("ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:", ("PASCAL VOC", "COCO", "YOLO Darknet"), key="detection_format_select")
            options_with_counts = [f"{label} ({count}æš)" for label, count in available_labels]
            selected_options = st.multiselect("å«ã‚ã‚‹ç‰©ä½“ãƒ©ãƒ™ãƒ« (æœªé¸æŠ=å…¨ã¦):", options_with_counts, key="detection_labels_multiselect")

            if st.button("æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ", key="generate_detection_dataset_btn"):
                selected_labels_to_filter = [parse_label_from_option(opt) for opt in selected_options]
                
                db = SessionLocal()
                try:
                    query = db.query(Image).filter(Image.annotations != None, Image.annotations != text("'[]'"))
                    if selected_labels_to_filter:
                        # '?' æ¼”ç®—å­ã¯jsonbé…åˆ—ã®ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ã‚­ãƒ¼ã®å­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
                        # å„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®'label_name'ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã«ã¯@>æ¼”ç®—å­ãªã©ãŒã‚ˆã‚Šé©åˆ‡
                        # ã“ã“ã§ã¯ ANY ã¨ ->> ã‚’ä½¿ã£ã¦é…åˆ—å†…ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ãƒã‚§ãƒƒã‚¯
                         query = query.filter(text("EXISTS (SELECT 1 FROM jsonb_array_elements(images.annotations) as elem WHERE elem->>'label_name' = ANY(:labels))").bindparams(labels=selected_labels_to_filter))

                    images_to_export = query.all()
                    
                    if not images_to_export:
                        st.warning("å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return

                    with st.spinner(f"{len(images_to_export)}ä»¶ã®ç”»åƒã‚’å‡¦ç†ä¸­..."):
                        import tempfile; import zipfile
                        with tempfile.TemporaryDirectory() as tmpdir:
                            dataset_root = Path(tmpdir) / "dataset"
                            (dataset_root / "images").mkdir(parents=True)
                            (dataset_root / "annotations").mkdir()
                            
                            gen_func = {"PASCAL VOC": generate_pascal_voc_dataset, "COCO": generate_coco_dataset, "YOLO Darknet": generate_yolo_dataset}[dataset_format]
                            # â˜…â˜…â˜… ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ â˜…â˜…â˜…
                            # cursorã‚„idãƒªã‚¹ãƒˆã®ä»£ã‚ã‚Šã«ã€Imageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã‚’ç›´æ¥æ¸¡ã™
                            generated_count, missing_files = gen_func(images_to_export, dataset_root)
                            
                            if generated_count == 0:
                                st.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒãªã‹ã£ãŸãŸã‚ã€zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"); return
                            
                            zip_path_str = os.path.join(tmpdir, "export"); shutil.make_archive(zip_path_str, 'zip', dataset_root)
                            zip_filename = f"{dataset_format.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                            st.download_button(label=f"ğŸ“¥ {dataset_format} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’DL", data=Path(f"{zip_path_str}.zip").read_bytes(), file_name=zip_filename, mime="application/zip")
                            if missing_files: st.warning(f"{len(missing_files)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¬ æã¾ãŸã¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã§ã—ãŸ: {', '.join(missing_files[:5])}...")
                except Exception as e:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    st.error(traceback.format_exc())
                finally:
                    db.close()

    # --- ç”»åƒåˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã‚¿ãƒ– ---
    with tab2:
        st.info("ç‰¹å®šã®åˆ†é¡ãƒ©ãƒ™ãƒ«ãŒä»˜ä¸ã•ã‚ŒãŸç”»åƒã‚’æŠ½å‡ºã—ã€ãƒ•ã‚©ãƒ«ãƒ€åˆ†ã‘ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚")
        db = SessionLocal()
        try:
            query = db.query(Image.label, func.count(Image.id)).filter(Image.label != None, Image.label != '').group_by(Image.label).order_by(Image.label)
            available_labels = query.all()
        except Exception as e:
            st.error(f"åˆ†é¡ãƒ©ãƒ™ãƒ«ã®é›†è¨ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"); available_labels = []
        finally:
            db.close()
        
        if not available_labels:
            st.warning("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åˆ†é¡ãƒ©ãƒ™ãƒ«ä»˜ãã®ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            options_with_counts = [f"{label} ({count}æš)" for label, count in available_labels]
            selected_options = st.multiselect("å«ã‚ã‚‹åˆ†é¡ãƒ©ãƒ™ãƒ« (æœªé¸æŠ=å…¨ã¦):", options_with_counts, key="classification_labels_multiselect")
            
            if st.button("åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ", key="generate_classification_dataset_btn"):
                labels_to_filter = [parse_label_from_option(opt) for opt in selected_options]
                if not labels_to_filter:
                    # æœªé¸æŠã®å ´åˆã¯å…¨ã¦ã®ãƒ©ãƒ™ãƒ«ã‚’å¯¾è±¡ã¨ã™ã‚‹
                    labels_to_filter = [label for label, count in available_labels]

                db = SessionLocal()
                try:
                    images_to_export = db.query(Image).filter(Image.label.in_(labels_to_filter)).all()
                    if not images_to_export:
                        st.warning("å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return

                    with st.spinner(f"å‡¦ç†ä¸­... {len(images_to_export)}ä»¶"):
                        import tempfile; import zipfile
                        with tempfile.TemporaryDirectory() as tmpdir:
                            dataset_root = Path(tmpdir) / "classification_dataset"
                            dataset_root.mkdir()
                            missing_files_count = 0
                            
                            for image_obj in images_to_export:
                                label_dir = dataset_root / image_obj.label
                                label_dir.mkdir(exist_ok=True)
                                local_path = get_image_from_s3(image_obj.s3_key, str(tmpdir))
                                if local_path:
                                    shutil.copy(local_path, label_dir / image_obj.filename)
                                else:
                                    missing_files_count += 1
                            
                            if len(images_to_export) == missing_files_count:
                                st.warning("æœ‰åŠ¹ãªç”»åƒãƒ‡ãƒ¼ã‚¿ãŒãªã‹ã£ãŸãŸã‚ã€zipã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"); return

                            zip_path_str = os.path.join(tmpdir, "export"); shutil.make_archive(zip_path_str, 'zip', dataset_root)
                            zip_filename = f"classification_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                            st.download_button(label="ğŸ“¥ åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’DL", data=Path(f"{zip_path_str}.zip").read_bytes(), file_name=zip_filename, mime="application/zip")
                            if missing_files_count > 0: st.warning(f"{missing_files_count}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒS3ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                except Exception as e:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    st.error(traceback.format_exc())
                finally:
                    db.close()

def main():
    st.set_page_config(layout="wide")
    cleanup_old_session_dirs()

    if 'current_page' not in st.session_state:
        st.session_state.current_page = "ç”»åƒã‚’ç™»éŒ²ã™ã‚‹"
    if 'last_activity_time' not in st.session_state:
        st.session_state.last_activity_time = time.time()
    if 'temp_dir_session' not in st.session_state or not os.path.exists(st.session_state.temp_dir_session):
        st.session_state.temp_dir_session = setup_session_temp_dir()

    if check_session_timeout():
        return

    try:
        logo_path = "./KG-Motors-Logo.png"
        if os.path.exists(logo_path):
            st.sidebar.image(logo_path, width=150) 
        else:
            st.sidebar.warning(f"ãƒ­ã‚´ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {os.path.abspath(logo_path)}")
    except Exception as e_logo:
        st.sidebar.error(f"ãƒ­ã‚´ç”»åƒã®èª­ã¿è¾¼ã¿/è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_logo}")

    st.sidebar.title("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
    
    page_option_register = "ç”»åƒã‚’ç™»éŒ²ã™ã‚‹"
    page_option_search = "ç”»åƒã‚’æ¤œç´¢ã™ã‚‹"
    page_option_dataset = "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"

    if st.sidebar.button(page_option_register, key="nav_button_register", use_container_width=True):
        st.session_state.current_page = page_option_register
        st.session_state.last_activity_time = time.time()
        st.rerun()

    if st.sidebar.button(page_option_search, key="nav_button_search", use_container_width=True):
        st.session_state.current_page = page_option_search
        st.session_state.last_activity_time = time.time()
        st.rerun()

    if st.sidebar.button(page_option_dataset, key="nav_button_dataset", use_container_width=True):
        st.session_state.current_page = page_option_dataset
        st.session_state.last_activity_time = time.time()
        st.rerun()

    if 'authenticated' not in st.session_state:
        st.session_state['authenticated'] = True 

    if st.session_state.current_page == page_option_register:
        st.title("KGç”»åƒç™»éŒ²ã‚·ã‚¹ãƒ†ãƒ ") 
        convert_image_to_vector() 
    
    elif st.session_state.current_page == page_option_search:
        run_image_search_app()

    elif st.session_state.current_page == page_option_dataset:
        create_dataset_page()

if __name__ == "__main__":
    main()