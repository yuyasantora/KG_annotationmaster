import streamlit as st
import openai
import numpy as np
import pandas as pd
from PIL import Image as PILImage
from search import create_index_faiss, find_closest_vectors_faiss
from openai import OpenAI
import os
import subprocess
import io
import traceback
import shutil
import sqlite3
import time
import uuid
from pathlib import Path
import xml.etree.ElementTree as ET
import sys
from datetime import datetime, timedelta
import cv2
from yolox.yolox_onnx_predictor import YOLOXONNXPredictor
import json
import re
from dotenv import load_dotenv
import boto3
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy import Column, Integer, String, JSON
from sqlalchemy import func, text, or_
from sqlalchemy.dialects.postgresql import JSONB
from sentence_transformers import SentenceTransformer
import faiss


load_dotenv()

DB_PATH = "annotation_data.db"

# S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
S3_BUCKET_NAME = os.getenv("AWS_S3_BUCKET_NAME")
s3 = boto3.client(
    "s3",
    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    region_name=os.getenv("AWS_S3_REGION")
)
# --- â˜… è¨­å®šã“ã“ã¾ã§ â˜… ---

TEMP_SESSIONS_BASE_DIR = "temp_sessions"
OLD_SESSION_MAX_AGE_HOURS = 24 # èµ·å‹•æ™‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ï¼ˆä¾‹: 24æ™‚é–“ï¼‰
CURRENT_SESSION_TIMEOUT_SECONDS = 3600 # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆä¾‹: 1æ™‚é–“ = 3600ç§’ï¼‰

# äº‹å‰å®šç¾©ãƒ¢ãƒ‡ãƒ«
PRESET_MODELS = {
    "ä¿¡å·æ¤œå‡º":{
        "path":"yolox/onnx_models/tlr_car_ped_yolox_s_batch_1.onnx", # ãƒãƒƒãƒã‚µã‚¤ã‚º1ã®ãƒ¢ãƒ‡ãƒ«ã‚’æƒ³å®š
        "input_shape":"416,416",
        "class_names":[ "pedestrian_traffic_light", "traffic_light"]
    },
}

# --- SQLAlchemyãƒ¢ãƒ‡ãƒ«å®šç¾© ---

# ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ãŒç¶™æ‰¿ã™ã‚‹ãŸã‚ã®Baseã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
Base = declarative_base()

# ç”»åƒæƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«
class Image(Base):
    __tablename__ = 'images'
    id = Column(Integer, primary_key=True, index=True)
    filename = Column(String, unique=True, index=True)
    s3_key = Column(String, unique=True)
    label = Column(String, nullable=True)
    annotations = Column(JSONB, nullable=True)
    width = Column(Integer, nullable=True)
    height = Column(Integer, nullable=True)
    vector = Column(JSONB, nullable=True)

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šè¨­å®š ---
# å…ƒã®SQLiteæ¥ç¶šè¨­å®š
# DATABASE_FILE = "annotation.db"
# engine = create_engine(f"sqlite:///{DATABASE_FILE}")

# æ–°ã—ã„PostgreSQLã¸ã®æ¥ç¶šè¨­å®š
DATABASE_URL = os.getenv("DATABASE_URL")
if DATABASE_URL is None:
    st.error("DATABASE_URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

engine = create_engine(DATABASE_URL)

# --- ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ ---
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆå›ä½œæˆã™ã‚‹
Base.metadata.create_all(engine)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def parse_pascal_voc_xml(xml_path):
    tree = ET.parse(xml_path)
    root = tree.getroot()
    objects = []
    for obj in root.findall('object'):
        label = obj.find('name').text
        bbox = obj.find('bndbox')
        xmin = int(bbox.find('xmin').text)
        ymin = int(bbox.find('ymin').text)
        xmax = int(bbox.find('xmax').text)
        ymax = int(bbox.find('ymax').text)
        objects.append({'label_name': label, 'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax})
    return objects

def get_classes_from_xml_dir(xml_dir_path):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®PASCAL VOC XMLã‹ã‚‰ã‚¯ãƒ©ã‚¹åã‚’æŠ½å‡ºã™ã‚‹"""
    annotated_classes = set()
    if not os.path.isdir(xml_dir_path):
        return annotated_classes
    
    for filename in os.listdir(xml_dir_path):
        if filename.endswith(".xml"):
            xml_file_path = os.path.join(xml_dir_path, filename)
            try:
                tree = ET.parse(xml_file_path)
                root = tree.getroot()
                for obj in root.findall("object"):
                    name_element = obj.find("name")
                    if name_element is not None and name_element.text:
                        annotated_classes.add(name_element.text.strip())
            except ET.ParseError:
                # ä¸æ­£ãªXMLã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
    return annotated_classes

def format_caption(rank, description):
    return f"**Rank {rank}:**\n{description}"

@st.cache_data
def load_embedding_model():
    """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    return SentenceTransformer('clip-ViT-B-32')

def generate_embedding(image: PILImage.Image, model):
    """PILç”»åƒã‚’å—ã‘å–ã‚Šã€ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ï¼‰ã‚’è¿”ã™"""
    embedding = model.encode(image)
    return embedding.tolist()



def search_images_in_db(search_term):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ç”»åƒã‚’æ¤œç´¢ã—ã€çµæœã‚’è¿”ã™ã€‚çµæœã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ã€‚"""
    db = SessionLocal()
    try:
        query = db.query(Image)
        if search_term:
            search_pattern = f'%{search_term}%'
            from sqlalchemy import or_
            query = query.filter(
                or_(
                    Image.filename.ilike(search_pattern),
                    Image.label.ilike(search_pattern)
                )
            )
        return query.order_by(Image.id.desc()).all()
    finally:
        db.close()

def run_image_search_app():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ã•ã‚ŒãŸç”»åƒã®ä¸€è¦§è¡¨ç¤ºã¨æ¤œç´¢ã‚’è¡Œã†ãƒšãƒ¼ã‚¸"""
    st.header("ç™»éŒ²æ¸ˆã¿ç”»åƒã®æ¤œç´¢ãƒ»é–²è¦§")
    st.info("ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ—¥æœ¬èªå¯ï¼‰ã§ã€é–¢é€£æ€§ã®é«˜ã„ç”»åƒã‚’æ¤œç´¢ã—ã¾ã™ã€‚ä¾‹ï¼šã€Œèµ¤è‰²ã®ä¹—ç”¨è»Šã€ã€Œäº¤å·®ç‚¹ã‚’æ­©ãäººã€…ã€")

    # --- æ¤œç´¢UI ---
    search_term = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", key="faiss_search_term")

    # --- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ ---
    st.subheader("æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†")
    if st.button("æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰ã™ã‚‹"):
        with st.spinner("ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™..."):
            build_and_save_faiss_index()

    st.markdown("---")

    # --- æ¤œç´¢å®Ÿè¡Œã¨çµæœè¡¨ç¤º ---
    if search_term:
        results = search_images_with_faiss(search_term)

        if not results:
            st.info("æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
            
        st.write(f"**{len(results)}** ä»¶ã®ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ï¼ˆé¡ä¼¼åº¦ãŒé«˜ã„é †ï¼‰")

        cols = st.columns(4)
        for i, img_obj in enumerate(results):
            with cols[i % 4]:
                image_bytes = get_image_bytes_from_s3(img_obj.s3_key)
                
                if image_bytes:
                    st.image(image_bytes, caption=f"é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {i+1}", use_container_width=True)
                    st.markdown(f"**{img_obj.filename}**")
                    st.caption(f"åˆ†é¡: {img_obj.label or '(æœªè¨­å®š)'}")
                    anno_count = len(img_obj.annotations) if img_obj.annotations else 0
                    st.caption(f"ç‰©ä½“æ•°: {anno_count}")
                else:
                    st.warning(f"è¡¨ç¤ºä¸å¯:\n{img_obj.filename}")

def setup_session_temp_dir():
    base_temp_path = Path(TEMP_SESSIONS_BASE_DIR)
    base_temp_path.mkdir(exist_ok=True)
    session_dir_name = f"session_{str(uuid.uuid4())[:8]}"
    session_temp_path = base_temp_path / session_dir_name
    session_temp_path.mkdir(exist_ok=True, parents=True)
    
    if 'last_activity_time' in st.session_state:
        st.session_state.last_activity_time = time.time()
    else:
        st.session_state.last_activity_time = time.time()
    return str(session_temp_path)

def cleanup_session_temp_dir(session_dir_path):
    if session_dir_path and os.path.exists(session_dir_path):
        try:
            shutil.rmtree(session_dir_path)
        except Exception as e:
            st.error(f"ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {session_dir_path}, Error: {e}")

def generate_pascal_voc_dataset(images_to_export: list[Image], tmpdir_path: Path):
    """
    æ¤œå‡ºçµæœã‹ã‚‰PASCAL VOC XMLæ–‡å­—åˆ—ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    objects: [{'label_name': str, 'xmin': int, ...}, ...]
    """
    missing_files = []

    image_dir = tmpdir_path / "images"
    annotations_dir = tmpdir_path / "annotations"

    for image_obj in images_to_export:
        local_image_path = get_image_from_s3(image_obj.s3_key, str(tmpdir_path))
        if not local_image_path:
            missing_files.append(image_obj.filename)
            continue

        try:
            with PILImage.open(local_image_path) as img:
                width, height = img.size
        except Exception as e:
            st.warning(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«{image_obj.filename}ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚{e}")
            missing_files.append(image_obj.filename)
            continue

        shutil.copy(local_image_path, image_dir / image_obj.filename)

        # ã‚³ãƒ”ãƒ¼å…ƒã®ä¸€æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹
        os.remove(local_image_path)

        detected_objects = image_obj.annotations if image_obj.annotations else []
        xml_content = generate_pascal_voc_xml_content(image_obj.filename, local_image_path, width, height, detected_objects)
        xml_filename = Path(image_obj.filename).stem + ".xml"
        with open(annotations_dir / xml_filename, "w", encoding="utf-8") as f:
            f.write(xml_content)

    return len(images_to_export) - len(missing_files), missing_files

def generate_coco_dataset(images_to_export: list[Image], tmpdir_path: Path):
    """
    æ¤œå‡ºçµæœã‹ã‚‰COCO JSONæ–‡å­—åˆ—ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    
    # å…¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’åé›†
    all_labels = sorted(list(set(
        obj["label_name"] for img in images_to_export if img.annotations for obj in img.annotations

    )))
    categories = [{"id": i+1, "name": label, "supercategory": "none"} for i, label in enumerate(all_labels)]
    category_map = {cat["name"]: cat["id"] for cat in categories}

    coco_output = {
        "info": {"description": "Generated by Annotation Tool", "version": "1.0", "year": datetime.now().year, "date_created": datetime.now().strftime('%Y/%m/%d')},
        "licenses": [], "images": [], "annotations": [], "categories": categories
    }

    annotation_id_counter = 1
    missing_files = []
    images_dir = tmpdir_path / "images"

    for image_obj in images_to_export:
        local_image_path = get_image_from_s3(image_obj.s3_key, str(tmpdir_path))
        if not local_image_path:
            missing_files.append(image_obj.filename)
            continue
            
        try:
            with PILImage.open(local_image_path) as img:
                width, height = img.size
        except Exception as e:
            st.warning(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ« {image_obj.filename} ã‚’é–‹ã‘ã¾ã›ã‚“: {e}")
            missing_files.append(image_obj.filename)
            continue

        shutil.copy(local_image_path, images_dir / image_obj.filename)

        os.remove(local_image_path)


        
        image_coco_id = len(coco_output["images"]) + 1
        coco_output["images"].append({"id": image_coco_id, "file_name": image_obj.filename, "width": width, "height": height})
        
        if image_obj.annotations:
            for anno in image_obj.annotations:
                xmin, ymin, xmax, ymax = anno['xmin'], anno['ymin'], anno['xmax'], anno['ymax']
                bbox_width, bbox_height = xmax - xmin, ymax - ymin
                if anno['label_name'] in category_map:
                    coco_output["annotations"].append({
                        "id": annotation_id_counter, "image_id": image_coco_id, 
                        "category_id": category_map[anno['label_name']], 
                        "bbox": [xmin, ymin, bbox_width, bbox_height], 
                        "area": bbox_width * bbox_height, "iscrowd": 0, "segmentation": []
                    })
                    annotation_id_counter += 1
    
    with open(tmpdir_path / "annotations" / "instances.json", "w") as f:
        json.dump(coco_output, f, indent=4)
        
    return len(images_to_export) - len(missing_files), missing_files

def generate_yolo_dataset(images_to_export: list[Image], tmpdir_path: Path):
    """YOLO Darknetå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã€çµ±è¨ˆæƒ…å ±ã‚’è¿”ã™"""
    st.info("YOLO Darknetå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
    
    all_labels = sorted(list(set(
        obj['label_name'] 
        for img in images_to_export if img.annotations 
        for obj in img.annotations
    )))
    class_map = {label: i for i, label in enumerate(all_labels)}
    with open(tmpdir_path / "classes.txt", "w") as f:
        for label in all_labels: f.write(f"{label}\n")

    missing_files = []
    images_dir = tmpdir_path / "images"
    annotations_dir = tmpdir_path / "annotations"

    for image_obj in images_to_export:
        local_image_path = get_image_from_s3(image_obj.s3_key, str(tmpdir_path))
        if not local_image_path:
            missing_files.append(image_obj.filename)
            continue

        try:
            with PILImage.open(local_image_path) as img:
                img_width, img_height = img.size
        except Exception as e:
            st.warning(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ« {image_obj.filename} ã‚’é–‹ã‘ã¾ã›ã‚“: {e}")
            missing_files.append(image_obj.filename)
            continue
            
        shutil.copy(local_image_path, images_dir / image_obj.filename)

        os.remove(local_image_path)
        
        yolo_lines = []
        if image_obj.annotations:
            for anno in image_obj.annotations:
                class_id = class_map[anno['label_name']]
                xmin, ymin, xmax, ymax = anno['xmin'], anno['ymin'], anno['xmax'], anno['ymax']
                x_center = (xmin + xmax) / 2 / img_width
                y_center = (ymin + ymax) / 2 / img_height
                width = (xmax - xmin) / img_width
                height = (ymax - ymin) / img_height
                yolo_lines.append(f"{class_id} {x_center} {y_center} {width} {height}")
        
        txt_filename = Path(image_obj.filename).stem + ".txt"
        with open(annotations_dir / txt_filename, "w") as f:
            f.write("\n".join(yolo_lines))
            
    return len(images_to_export) - len(missing_files), missing_files

def get_image_from_s3(s3_key, temp_dir):
    """S3ã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿”ã™"""
    if not s3_key:
        return None
    try:
        local_path = os.path.join(temp_dir, os.path.basename(s3_key))
        s3.download_file(S3_BUCKET_NAME, s3_key, local_path)
        return local_path
    except Exception as e:
        st.warning(f"S3ã‹ã‚‰ã®ç”»åƒå–å¾—ã«å¤±æ•—: {s3_key}, Error: {e}")
        return None

@st.cache_data
def get_image_bytes_from_s3(s3_key):
    """S3ã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã€‚çµæœã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ã€‚"""
    if not s3_key:
        return None
    try:
        s3_object = s3.get_object(Bucket=S3_BUCKET_NAME, Key=s3_key)
        return s3_object['Body'].read()
    except Exception as e:
        st.warning(f"S3ã‹ã‚‰ã®ç”»åƒå–å¾—ã«å¤±æ•—: {s3_key}, Error: {e}")
        return None

@st.cache_data
def get_bytes_from_uploaded_file(uploaded_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã€‚çµæœã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ã€‚"""
    return uploaded_file.getvalue()

def generate_pascal_voc_xml_content(filename, path, width, height, objects):
    """
    æ¤œå‡ºçµæœã‹ã‚‰PASCAL VOC XMLã®æ–‡å­—åˆ—ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    objects: [{'label_name': str, 'xmin': int, ...}, ...]
    """
    annotation = ET.Element('annotation')
    ET.SubElement(annotation, 'folder').text = "images"
    ET.SubElement(annotation, 'filename').text = filename
    ET.SubElement(annotation, 'path').text = str(path)
    source = ET.SubElement(annotation, 'source')
    ET.SubElement(source, 'database').text = 'Unknown'
    
    size = ET.SubElement(annotation, 'size')
    ET.SubElement(size, 'width').text = str(width)
    ET.SubElement(size, 'height').text = str(height)
    ET.SubElement(size, 'depth').text = '3'
    ET.SubElement(annotation, 'segmented').text = '0'

    # objectsãŒNoneã‚„ç©ºãƒªã‚¹ãƒˆã®å ´åˆã§ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã‚ˆã†ã«ãƒã‚§ãƒƒã‚¯
    if objects:
        for obj_dict in objects:
            obj = ET.SubElement(annotation, 'object')
            ET.SubElement(obj, 'name').text = obj_dict['label_name']
            ET.SubElement(obj, 'pose').text = 'Unspecified'
            ET.SubElement(obj, 'truncated').text = '0'
            ET.SubElement(obj, 'difficult').text = '0'
            bndbox = ET.SubElement(obj, 'bndbox')
            ET.SubElement(bndbox, 'xmin').text = str(obj_dict['xmin'])
            ET.SubElement(bndbox, 'ymin').text = str(obj_dict['ymin'])
            ET.SubElement(bndbox, 'xmax').text = str(obj_dict['xmax'])
            ET.SubElement(bndbox, 'ymax').text = str(obj_dict['ymax'])

    # ElementTreeã‹ã‚‰æ•´å½¢ã•ã‚ŒãŸXMLæ–‡å­—åˆ—ã‚’ç”Ÿæˆ
    ET.indent(annotation, space="  ")
    xml_string = ET.tostring(annotation, encoding='unicode')
    
    return f'<?xml version="1.0" ?>\n{xml_string}'

@st.cache_resource
def load_embedding_model():
    """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    return SentenceTransformer('clip-ViT-B-32')

def generate_embedding(image: PILImage.Image, model):
    """PILç”»åƒã‚’å—ã‘å–ã‚Šã€ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ï¼‰ã‚’è¿”ã™"""
    embedding = model.encode(image)
    return embedding.tolist()

def convert_image_to_vector():
    # --- ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ ---
    model = load_embedding_model()

    # --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†å¤‰æ•°ã®åˆæœŸåŒ– ---
    if 'staged_files' not in st.session_state:
        st.session_state.staged_files = {} # filename: UploadedFile
    if 'annotation_data' not in st.session_state:
        st.session_state.annotation_data = {} # filename: {annotations: [], label: ""}

    # --- ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™ ---
    if 'temp_dir_session' not in st.session_state or not os.path.exists(st.session_state.temp_dir_session):
        st.session_state.temp_dir_session = setup_session_temp_dir()
    temp_dir = st.session_state.temp_dir_session
    local_image_storage_dir = os.path.join(temp_dir, "images_for_labeling")
    open_labeling_output_dir_base = os.path.join(temp_dir, "annotations_output")
    open_labeling_pascal_voc_output_dir = os.path.join(open_labeling_output_dir_base, "PASCAL_VOC")
    os.makedirs(local_image_storage_dir, exist_ok=True)
    os.makedirs(open_labeling_pascal_voc_output_dir, exist_ok=True)

    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    st.header("1. ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["png", "jpg", "jpeg"], accept_multiple_files=True, key="file_uploader_main")
    if uploaded_files:
        new_files_added = False
        for f in uploaded_files:
            if f.name not in st.session_state.staged_files:
                st.session_state.staged_files[f.name] = f
                # å¯¾å¿œã™ã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚‚åˆæœŸåŒ–
                st.session_state.annotation_data[f.name] = {"annotations": [], "label": ""}
                new_files_added = True
        if new_files_added:
            st.rerun()

    if not st.session_state.staged_files:
        st.info("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ã“ã“ã«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        st.stop()

    # --- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ---
    st.subheader(f"{len(st.session_state.staged_files)} ä»¶ã®ç”»åƒãŒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ä¸­")
    with st.expander("ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’é–‹ã/é–‰ã˜ã‚‹"):
        cols = st.columns(4)
        for i, (filename, file_data) in enumerate(st.session_state.staged_files.items()):
            with cols[i % 4]:
                image_bytes = get_bytes_from_uploaded_file(file_data)
                st.image(image_bytes, caption=filename, use_container_width=True)

    st.markdown("---")

    # --- 2. è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ---
    st.header("2. è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ (ä»»æ„)")
    auto_annotate_method = st.radio("æ–¹æ³•ã‚’é¸æŠ", ("ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨", "ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"), key="auto_annotate_method_radio")

    onnx_model_bytes, class_names, input_shape = None, None, "640,640"
    if auto_annotate_method == "ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨" and PRESET_MODELS:
        model_name = st.selectbox("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ", options=list(PRESET_MODELS.keys()))
        model_info = PRESET_MODELS[model_name]
        if os.path.exists(model_info["path"]):
            with open(model_info["path"], "rb") as f:
                onnx_model_bytes = f.read()
            class_names = model_info.get("class_names")
            input_shape = model_info.get("input_shape", "640,640")
        else:
            st.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_info['path']}")
    elif auto_annotate_method == "ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        f = st.file_uploader("ONNXãƒ¢ãƒ‡ãƒ« (.onnx)", type=["onnx"])
        if f: onnx_model_bytes = f.read()

    confidence_threshold = st.slider("ä¿¡é ¼åº¦ã®ã—ãã„å€¤", 0.0, 1.0, 0.3, 0.05)
    
    if st.button("è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"):
        if onnx_model_bytes:
            try:
                predictor = YOLOXONNXPredictor(model_bytes=onnx_model_bytes, input_shape_str=input_shape, class_names=class_names)
                with st.spinner("è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œä¸­..."):
                    for filename, file_data in st.session_state.staged_files.items():
                        image_bytes = get_bytes_from_uploaded_file(file_data)
                        np_array = np.frombuffer(image_bytes, np.uint8)
                        cv_image = cv2.imdecode(np_array, cv2.IMREAD_COLOR)
                        
                        try:
                            # äºˆæ¸¬ã‚’å®Ÿè¡Œ
                            detected = predictor.predict(cv_image, score_thr=confidence_threshold)
                        except IndexError:
                            # `predict`é–¢æ•°ãŒå†…éƒ¨ã§ç©ºã®ãƒªã‚¹ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã‚ˆã†ã¨ã—ã¦ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å ´åˆã‚’æƒ³å®šã€‚
                            # ã“ã®ã‚¨ãƒ©ãƒ¼ã¯ã€Œä½•ã‚‚æ¤œå‡ºã—ãªã‹ã£ãŸã€ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã¨ã¿ãªã—ã€
                            # ç©ºã®ãƒªã‚¹ãƒˆ `[]` ã‚’çµæœã¨ã—ã¦æ‰±ã†ã“ã¨ã§ã€å‡¦ç†ã‚’ç¶šè¡Œã•ã›ã‚‹ã€‚
                            detected = []
                        
                        # çµæœã‚’XMLã¨ã—ã¦ä¿å­˜
                        pil_img = PILImage.open(io.BytesIO(image_bytes))
                        xml_content = generate_pascal_voc_xml_content(filename, "s3_path_placeholder", pil_img.width, pil_img.height, detected)
                        with open(os.path.join(open_labeling_pascal_voc_output_dir, f"{Path(filename).stem}.xml"), "w") as f:
                            f.write(xml_content)
                st.success("è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ä¸‹è¨˜OpenLabelingã§çµæœã‚’ç¢ºèªãƒ»ä¿®æ­£ã§ãã¾ã™ã€‚")
            except Exception as e:
                st.error(f"è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            st.warning("ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã¾ãŸã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    st.markdown("---")

    # --- 3. æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ---
    st.header("3. æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ (OpenLabeling)")
    with st.expander("ğŸ“– OpenLabelingã®ç°¡å˜ãªä½¿ã„æ–¹ã‚¬ã‚¤ãƒ‰"):
        st.markdown("""
            **OpenLabelingã¯åˆ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§é–‹ã‹ã‚Œã¾ã™ã€‚**
            
            1.  **ç”»åƒã®åˆ‡ã‚Šæ›¿ãˆ:**
                *   `d` ã‚­ãƒ¼: æ¬¡ã®ç”»åƒã¸
                *   `a` ã‚­ãƒ¼: å‰ã®ç”»åƒã¸
            
            2.  **ãƒœãƒƒã‚¯ã‚¹ã®ä½œæˆ:**
                *   ç”»åƒã®ä¸Šã§ãƒã‚¦ã‚¹ã‚’ãƒ‰ãƒ©ãƒƒã‚°ã—ã¦ã€ç‰©ä½“ã‚’å›²ã‚€å››è§’å½¢ï¼ˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ï¼‰ã‚’æç”»ã—ã¾ã™ã€‚
            
            3.  **ã‚¯ãƒ©ã‚¹ã®é¸æŠ:**
                *   ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»ã™ã‚‹ã¨ã€ã‚¯ãƒ©ã‚¹ã‚’é¸æŠã™ã‚‹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
                *   å…ˆã»ã©ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã§ç·¨é›†ã—ãŸã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆã®ä¸­ã‹ã‚‰ã€å¯¾å¿œã™ã‚‹ç‰©ä½“ã®ã‚¯ãƒ©ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
            
            4.  **ãƒœãƒƒã‚¯ã‚¹ã®ä¿®æ­£:**
                *   **ç§»å‹•:** ãƒœãƒƒã‚¯ã‚¹ã®ä¸­ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ‰ãƒ©ãƒƒã‚°
                *   **ã‚µã‚¤ã‚ºå¤‰æ›´:** ãƒœãƒƒã‚¯ã‚¹ã®ç«¯ã‚„è§’ã‚’ãƒ‰ãƒ©ãƒƒã‚°
                *   **å‰Šé™¤:** ãƒœãƒƒã‚¯ã‚¹ã‚’é¸æŠã—ã¦`Delete`ã‚­ãƒ¼
                *   **ã‚¯ãƒ©ã‚¹å¤‰æ›´:** ãƒœãƒƒã‚¯ã‚¹ã‚’ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯
            
            5.  **ä¿å­˜:**
                *   **éå¸¸ã«é‡è¦:** ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä»˜ã‘ãŸã‚‰ã€å¿…ãš `Ctrl + S` ã‚’æŠ¼ã—ã¦ä¿å­˜ã—ã¦ãã ã•ã„ã€‚ä¿å­˜ã—ãªã„ã¨ã€ä»˜ã‘ãŸã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯æ¶ˆãˆã¦ã—ã¾ã„ã¾ã™ã€‚
            
            6.  **çµ‚äº†:**
                *   å…¨ã¦ã®ç”»åƒã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒçµ‚ã‚ã£ãŸã‚‰ã€OpenLabelingã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‰ã˜ã¦ãã ã•ã„ã€‚
        """)

    # ã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆã®æº–å‚™
    class_list_file_path = os.path.abspath(os.path.join("OpenLabeling", "main", "class_list.txt"))
    default_classes_str = "object\nperson\nvehicle"
    try:
        if os.path.exists(class_list_file_path):
            with open(class_list_file_path, "r") as f: default_classes_str = f.read()
    except Exception: pass
    
    edited_classes_str = st.text_area("ã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆ (1è¡Œ1ã‚¯ãƒ©ã‚¹)", value=default_classes_str, height=150)

    if st.button("OpenLabelingã‚’èµ·å‹•"):
        # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ä¸­ã®ç”»åƒã‚’OpenLabelingã®å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€ã«æ›¸ãå‡ºã™
        for filename, file_data in st.session_state.staged_files.items():
            with open(os.path.join(local_image_storage_dir, filename), "wb") as f:
                f.write(get_bytes_from_uploaded_file(file_data))

        # ã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(class_list_file_path, "w") as f: f.write(edited_classes_str)

        command = [sys.executable, os.path.abspath("OpenLabeling/main/main.py"), "--input_dir", os.path.abspath(local_image_storage_dir), "--output_dir", os.path.abspath(open_labeling_output_dir_base), "--draw-from-PASCAL-files"]
        subprocess.Popen(command, cwd=os.path.abspath("OpenLabeling/main"))
        st.success("OpenLabelingãŒåˆ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§èµ·å‹•ã—ã¾ã—ãŸã€‚")

    st.markdown("---")
    
    # --- 4. åˆ†é¡ãƒ©ãƒ™ãƒ«å…¥åŠ›ã¨æœ€çµ‚ä¿å­˜ ---
    st.header("4. åˆ†é¡ãƒ©ãƒ™ãƒ«å…¥åŠ›ã¨DBã¸ã®ä¿å­˜")
    st.info("OpenLabelingã§ã®ä½œæ¥­ãŒå®Œäº†ã—ãŸã‚‰ã€ã“ã“ã§å„ç”»åƒã®åˆ†é¡ãƒ©ãƒ™ãƒ«ã‚’å…¥åŠ›ã—ã€ã™ã¹ã¦ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™ã€‚")
    
    # åˆ†é¡ãƒ©ãƒ™ãƒ«å…¥åŠ›æ¬„
    for filename in st.session_state.staged_files.keys():
        label = st.text_input(f"ã€Œ{filename}ã€ã®åˆ†é¡ãƒ©ãƒ™ãƒ«:", value=st.session_state.annotation_data.get(filename, {}).get("label", ""), key=f"cls_{filename}")
        st.session_state.annotation_data[filename]["label"] = label

    if st.button("å…¨ã¦ã‚’S3ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜", type="primary"):
        with st.spinner("ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³èª­è¾¼ã€S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€DBä¿å­˜ã‚’å®Ÿè¡Œä¸­..."):
            db = SessionLocal()
            try:
                for filename, file_data in st.session_state.staged_files.items():
                    # (1) ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³XMLã‚’èª­ã¿è¾¼ã‚€
                    xml_path = os.path.join(open_labeling_pascal_voc_output_dir, f"{Path(filename).stem}.xml")
                    if os.path.exists(xml_path):
                        st.session_state.annotation_data[filename]["annotations"] = parse_pascal_voc_xml(xml_path)
                    
                    # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ â˜…â˜…â˜…
                    # S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨PILã§ã®èª­ã¿è¾¼ã¿ã§ãƒ•ã‚¡ã‚¤ãƒ«ãŒç«¶åˆã—ãªã„ã‚ˆã†ã€
                    # å…ˆã«ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã‹ã‚‰ã€ãã‚Œãã‚Œã«ä½¿ç”¨ã™ã‚‹
                    file_data.seek(0) # ãƒã‚¤ãƒ³ã‚¿ã‚’æœ€åˆã«æˆ»ã™
                    image_bytes = file_data.read()
                    
                    # (2) S3ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                    s3_key = f"images/{uuid.uuid4()}_{filename}"
                    # BytesIOã‚’ä½¿ã£ã¦ãƒ¡ãƒ¢ãƒªä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦S3ã«æ¸¡ã™
                    s3.upload_fileobj(io.BytesIO(image_bytes), S3_BUCKET_NAME, s3_key, ExtraArgs={'ContentType': file_data.type})

                    # (3) DBã¸ä¿å­˜ (SQLAlchemyä½¿ç”¨)
                    pil_img = PILImage.open(io.BytesIO(image_bytes))
                    
                    # --- ã“ã“ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç† ---
                    # `image_vector` ã¨ã„ã†å¤‰æ•°ã«ã€ç”Ÿæˆã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜ã—ã¾ã™ã€‚
                    # ä¸Šã§å®šç¾©ã—ãŸ`generate_embedding`é–¢æ•°ã‚’ä½¿ã„ã€ç”»åƒ(pil_img)ã¨AIãƒ¢ãƒ‡ãƒ«(model)ã‚’æ¸¡ã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚
                    image_vector = generate_embedding(pil_img, model)
                    
                    # å­˜åœ¨ç¢ºèª (Upsert)
                    image_obj = db.query(Image).filter(Image.filename == filename).first()
                    
                    if image_obj: # æ›´æ–°
                        # S3ã«ã¯æ–°ã—ã„ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚­ãƒ¼ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã®ã§ã€DBã®s3_keyã‚‚æ›´æ–°ã™ã‚‹
                        image_obj.s3_key = s3_key
                        image_obj.filename = filename
                        image_obj.width = pil_img.width
                        image_obj.height = pil_img.height
                        image_obj.label = st.session_state.annotation_data[filename]["label"] or None
                        image_obj.annotations = st.session_state.annotation_data[filename]["annotations"]
                        image_obj.vector = image_vector # ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ›´æ–°
                    else: # æ–°è¦ä½œæˆ
                        image_obj = Image(
                            filename=filename,
                            s3_key=s3_key,
                            width=pil_img.width,
                            height=pil_img.height,
                            label=st.session_state.annotation_data[filename]["label"] or None,
                            annotations=st.session_state.annotation_data[filename]["annotations"],
                            vector=image_vector # ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
                        )
                        db.add(image_obj)
                
                db.commit()
                st.success(f"{len(st.session_state.staged_files)}ä»¶ã®ç”»åƒã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                # æ­£å¸¸ã«çµ‚äº†ã—ãŸã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªã‚¢
                st.session_state.staged_files.clear()
                st.session_state.annotation_data.clear()
                st.rerun()

            except Exception as e:
                db.rollback()
                st.error(f"ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.error(traceback.format_exc())
            finally:
                db.close()

    if st.session_state.staged_files:
        if st.button("ç¾åœ¨ã®å‡¦ç†ã‚’å…¨ã¦ã‚¯ãƒªã‚¢"):
            st.session_state.staged_files.clear()
            st.session_state.annotation_data.clear()
            st.rerun()

def cleanup_old_session_dirs():
    base_path = Path(TEMP_SESSIONS_BASE_DIR)
    if not base_path.exists():
        return
    now = datetime.now()
    for item in base_path.iterdir():
        if item.is_dir() and item.name.startswith("session_"):
            try:
                dir_modified_time = datetime.fromtimestamp(item.stat().st_mtime)
                if now - dir_modified_time > timedelta(hours=OLD_SESSION_MAX_AGE_HOURS):
                    shutil.rmtree(item)
            except Exception:
                pass 

def check_session_timeout():
    if 'last_activity_time' not in st.session_state:
        st.session_state.last_activity_time = time.time()
        return False

    if time.time() - st.session_state.last_activity_time > CURRENT_SESSION_TIMEOUT_SECONDS:
        current_session_dir_name = "ä¸æ˜ãªã‚»ãƒƒã‚·ãƒ§ãƒ³"
        if 'temp_dir_session' in st.session_state and st.session_state.temp_dir_session:
            current_session_dir_name = os.path.basename(st.session_state.temp_dir_session)
            cleanup_session_temp_dir(st.session_state.temp_dir_session)
        
        st.session_state.clear()
        st.session_state.temp_dir_session = setup_session_temp_dir()
        st.session_state.current_page = "ç”»åƒã‚’ç™»éŒ²ã™ã‚‹"
        
        st.info(f"é•·æ™‚é–“æ“ä½œãŒãªã‹ã£ãŸãŸã‚ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ ({current_session_dir_name}) ã‚’ã‚¯ãƒªã‚¢ã—ã€æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
        st.rerun()
        return True
    return False

def parse_label_from_option(option_str):
    """ "label (123æš)" ã¨ã„ã†å½¢å¼ã®æ–‡å­—åˆ—ã‹ã‚‰ãƒ©ãƒ™ãƒ«åã ã‘ã‚’æŠ½å‡ºã™ã‚‹ """
    match = re.match(r"^(.*) \(\d+æš\)$", option_str)
    if match:
        return match.group(1)
    return option_str

def create_dataset_page():
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚ã‚¿ãƒ–UIã‚’ä½¿ç”¨ã€‚"""
    st.header("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
    
    tab1, tab2 = st.tabs(["ç‰©ä½“æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", "ç”»åƒåˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"])

    # --- ç‰©ä½“æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã‚¿ãƒ– ---
    with tab1:
        st.info("ç‰¹å®šã®ç‰©ä½“ãƒ©ãƒ™ãƒ«ãŒå«ã¾ã‚Œã‚‹ç”»åƒã‚’æŠ½å‡ºã—ã€é¸æŠã—ãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚")
        
        db = SessionLocal()
        try:
            # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æŒã¤ç”»åƒã‹ã‚‰ãƒ©ãƒ™ãƒ«åã¨ã€ãã®ãƒ©ãƒ™ãƒ«ã‚’æŒã¤ç”»åƒã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ•°ã‚’é›†è¨ˆ
            subquery = db.query(
                Image.id.label("image_id"),
                func.jsonb_array_elements(Image.annotations).op('->>')('label_name').label("label_name")
            ).filter(
                Image.annotations.isnot(None),
                func.jsonb_array_length(Image.annotations) > 0 # text()ã‚’ä½¿ã‚ãªã„å½¢ã«ä¿®æ­£
            ).subquery()
            
            query = db.query(
                subquery.c.label_name,
                func.count(func.distinct(subquery.c.image_id))
            ).group_by(subquery.c.label_name).order_by(subquery.c.label_name)
            
            available_labels = query.all()
        except Exception as e:
            st.error(f"ç‰©ä½“ãƒ©ãƒ™ãƒ«ã®é›†è¨ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            available_labels = []
        finally:
            db.close()

        if not available_labels:
            st.warning("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç‰©ä½“ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            dataset_format = st.selectbox("ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:", ("PASCAL VOC", "COCO", "YOLO Darknet"), key="detection_format_select")
            options_with_counts = [f"{label} ({count}æš)" for label, count in available_labels]
            selected_options = st.multiselect("å«ã‚ã‚‹ç‰©ä½“ãƒ©ãƒ™ãƒ« (æœªé¸æŠ=å…¨ã¦):", options_with_counts, key="detection_labels_multiselect")

            if st.button("æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ", key="generate_detection_dataset_btn"):
                selected_labels_to_filter = [parse_label_from_option(opt) for opt in selected_options]
                
                db = SessionLocal()
                try:
                    query = db.query(Image).filter(
                        Image.annotations.isnot(None),
                        func.jsonb_array_length(Image.annotations) > 0
                    )
                    
                    if selected_labels_to_filter:
                        from sqlalchemy import or_
                        # annotations(JSONBé…åˆ—)ã«ã€æŒ‡å®šã•ã‚ŒãŸlabel_nameã‚’æŒã¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        # e.g., [{"label_name": "cat"}] ã¨ã„ã†JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒé…åˆ—å†…ã«å­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹
                        conditions = [
                            Image.annotations.op('@>')(json.dumps([{'label_name': label}])) 
                            for label in selected_labels_to_filter
                        ]
                        query = query.filter(or_(*conditions))

                    images_to_export = query.all()
                    
                    if not images_to_export:
                        st.warning("å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return

                    with st.spinner(f"{len(images_to_export)}ä»¶ã®ç”»åƒã‚’å‡¦ç†ä¸­..."):
                        import tempfile; import zipfile
                        with tempfile.TemporaryDirectory() as tmpdir:
                            dataset_root = Path(tmpdir) / "dataset"
                            (dataset_root / "images").mkdir(parents=True)
                            (dataset_root / "annotations").mkdir()
                            
                            gen_func = {
                                "PASCAL VOC": generate_pascal_voc_dataset, 
                                "COCO": generate_coco_dataset, 
                                "YOLO Darknet": generate_yolo_dataset
                            }[dataset_format]
                            
                            generated_count, missing_files = gen_func(images_to_export, dataset_root)
                            
                            if generated_count == 0:
                                st.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒãªã‹ã£ãŸãŸã‚ã€zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"); return
                            
                            zip_path_str = os.path.join(tmpdir, "export"); shutil.make_archive(zip_path_str, 'zip', dataset_root)
                            zip_filename = f"{dataset_format.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                            st.download_button(label=f"ğŸ“¥ {dataset_format} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’DL", data=Path(f"{zip_path_str}.zip").read_bytes(), file_name=zip_filename, mime="application/zip")
                            if missing_files: st.warning(f"{len(missing_files)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¬ æã¾ãŸã¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã§ã—ãŸ: {', '.join(missing_files[:5])}...")
                except Exception as e:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    st.error(traceback.format_exc())
                finally:
                    db.close()

    # --- ç”»åƒåˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã‚¿ãƒ– ---
    with tab2:
        st.info("ç‰¹å®šã®åˆ†é¡ãƒ©ãƒ™ãƒ«ãŒä»˜ä¸ã•ã‚ŒãŸç”»åƒã‚’æŠ½å‡ºã—ã€ãƒ•ã‚©ãƒ«ãƒ€åˆ†ã‘ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚")
        db = SessionLocal()
        try:
            query = db.query(Image.label, func.count(Image.id)).filter(
                Image.label.isnot(None), 
                Image.label != ''
            ).group_by(Image.label).order_by(Image.label)
            available_labels = query.all()
        except Exception as e:
            st.error(f"åˆ†é¡ãƒ©ãƒ™ãƒ«ã®é›†è¨ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"); available_labels = []
        finally:
            db.close()
        
        if not available_labels:
            st.warning("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åˆ†é¡ãƒ©ãƒ™ãƒ«ä»˜ãã®ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            options_with_counts = [f"{label} ({count}æš)" for label, count in available_labels]
            selected_options = st.multiselect("å«ã‚ã‚‹åˆ†é¡ãƒ©ãƒ™ãƒ« (æœªé¸æŠ=å…¨ã¦):", options_with_counts, key="classification_labels_multiselect")
            
            if st.button("åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ", key="generate_classification_dataset_btn"):
                labels_to_filter = [parse_label_from_option(opt) for opt in selected_options]
                if not labels_to_filter:
                    labels_to_filter = [label for label, count in available_labels]

                db = SessionLocal()
                try:
                    images_to_export = db.query(Image).filter(Image.label.in_(labels_to_filter)).all()
                    if not images_to_export:
                        st.warning("å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return

                    with st.spinner(f"å‡¦ç†ä¸­... {len(images_to_export)}ä»¶"):
                        import tempfile; import zipfile
                        with tempfile.TemporaryDirectory() as tmpdir:
                            dataset_root = Path(tmpdir) / "classification_dataset"
                            dataset_root.mkdir()
                            missing_files_count = 0
                            
                            for image_obj in images_to_export:
                                label_dir = dataset_root / image_obj.label
                                label_dir.mkdir(exist_ok=True)
                                local_path = get_image_from_s3(image_obj.s3_key, str(tmpdir))
                                if local_path:
                                    shutil.copy(local_path, label_dir / image_obj.filename)
                                else:
                                    missing_files_count += 1
                            
                            if len(images_to_export) == missing_files_count:
                                st.warning("æœ‰åŠ¹ãªç”»åƒãƒ‡ãƒ¼ã‚¿ãŒãªã‹ã£ãŸãŸã‚ã€zipã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"); return

                            zip_path_str = os.path.join(tmpdir, "export"); shutil.make_archive(zip_path_str, 'zip', dataset_root)
                            zip_filename = f"classification_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                            st.download_button(label="ğŸ“¥ åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’DL", data=Path(f"{zip_path_str}.zip").read_bytes(), file_name=zip_filename, mime="application/zip")
                            if missing_files_count > 0: st.warning(f"{missing_files_count}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒS3ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                except Exception as e:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    st.error(traceback.format_exc())
                finally:
                    db.close()

def build_and_save_faiss_index():
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®å…¨ç”»åƒãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚
    ç”»åƒIDã¨ã®å¯¾å¿œã‚‚ä¿å­˜ã™ã‚‹ã€‚
    """
    db = SessionLocal()
    try:
        images_with_vectors = db.query(Image.id, Image.vector).filter(Image.vector.isnot(None)).all()
        if not images_with_vectors:
            st.warning("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŒã¤ç”»åƒãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return

        image_ids = [img.id for img in images_with_vectors]
        vectors = np.array([img.vector for img in images_with_vectors]).astype('float32')

        if vectors.shape[0] == 0:
            st.warning("æœ‰åŠ¹ãªãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        dimension = vectors.shape[1]
        index = faiss.IndexFlatL2(dimension)  # L2è·é›¢ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼‰ã‚’ä½¿ç”¨
        index.add(vectors)

        faiss.write_index(index, "faiss.index")
        np.save("faiss_ids.npy", np.array(image_ids))

        st.success(f"{len(image_ids)}ä»¶ã®ç”»åƒã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ãƒ»ä¿å­˜ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        st.error(f"FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    finally:
        db.close()


def search_images_with_faiss(search_term, top_k=20):
    """
    FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã«é¡ä¼¼ã—ãŸç”»åƒã‚’æ¤œç´¢ã™ã‚‹ã€‚
    """
    try:
        index = faiss.read_index("faiss.index")
        image_ids = np.load("faiss_ids.npy")
    except FileNotFoundError:
        st.error("FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã€Œæ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        return []

    model = load_embedding_model()
    query_vector = model.encode([search_term]).astype('float32')

    # FAISSã§æ¤œç´¢å®Ÿè¡Œ
    distances, indices = index.search(query_vector, top_k)

    found_ids = [int(image_ids[i]) for i in indices[0] if i != -1]
    
    if not found_ids:
        return []

    db = SessionLocal()
    try:
        # è·é›¢ãŒè¿‘ã„é †ã«çµæœã‚’ä¸¦ã¹ã‚‹ãŸã‚ã€è¾æ›¸ã§è·é›¢ã‚’ä¿æŒ
        dist_map = {int(image_ids[i]): d for i, d in zip(indices[0], distances[0]) if i != -1}
        
        results = db.query(Image).filter(Image.id.in_(found_ids)).all()
        
        # æ¤œç´¢çµæœã‚’FAISSãŒè¿”ã—ãŸè·é›¢é †ã«ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda img: dist_map.get(img.id, float('inf')))
        
        return results
    finally:
        db.close()

def main():
    st.set_page_config(layout="wide")
    cleanup_old_session_dirs()

    if 'current_page' not in st.session_state:
        st.session_state.current_page = "ç”»åƒã‚’ç™»éŒ²ã™ã‚‹"
    if 'last_activity_time' not in st.session_state:
        st.session_state.last_activity_time = time.time()
    if 'temp_dir_session' not in st.session_state or not os.path.exists(st.session_state.temp_dir_session):
        st.session_state.temp_dir_session = setup_session_temp_dir()

    if check_session_timeout():
        return

    try:
        logo_path = "./KG-Motors-Logo.png"
        if os.path.exists(logo_path):
            st.sidebar.image(logo_path, width=150) 
        else:
            st.sidebar.warning(f"ãƒ­ã‚´ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {os.path.abspath(logo_path)}")
    except Exception as e_logo:
        st.sidebar.error(f"ãƒ­ã‚´ç”»åƒã®èª­ã¿è¾¼ã¿/è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_logo}")

    st.sidebar.title("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
    
    page_option_register = "ç”»åƒã‚’ç™»éŒ²ã™ã‚‹"
    page_option_search = "ç”»åƒã‚’æ¤œç´¢ã™ã‚‹"
    page_option_dataset = "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"

    if st.sidebar.button(page_option_register, key="nav_button_register", use_container_width=True):
        st.session_state.current_page = page_option_register
        st.session_state.last_activity_time = time.time()
        st.rerun()

    if st.sidebar.button(page_option_search, key="nav_button_search", use_container_width=True):
        st.session_state.current_page = page_option_search
        st.session_state.last_activity_time = time.time()
        st.rerun()

    if st.sidebar.button(page_option_dataset, key="nav_button_dataset", use_container_width=True):
        st.session_state.current_page = page_option_dataset
        st.session_state.last_activity_time = time.time()
        st.rerun()

    if 'authenticated' not in st.session_state:
        st.session_state['authenticated'] = True 

    if st.session_state.current_page == page_option_register:
        st.title("KGç”»åƒç™»éŒ²ã‚·ã‚¹ãƒ†ãƒ ") 
        convert_image_to_vector() 
    
    elif st.session_state.current_page == page_option_search:
        run_image_search_app()

    elif st.session_state.current_page == page_option_dataset:
        create_dataset_page()

if __name__ == "__main__":
    main()